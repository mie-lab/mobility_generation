{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from shapely import wkt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import trackintel as ti\n",
    "from trackintel.preprocessing.triplegs import generate_trips\n",
    "from trackintel.analysis.tracking_quality import temporal_tracking_quality, _split_overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file storage\n",
    "Dataset_file = os.path.join(\".\", \"paths.json\")\n",
    "with open(Dataset_file) as json_file:\n",
    "    CONFIG = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = pd.read_csv(os.path.join(CONFIG[f\"raw_mobis\"], \"sps.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp[\"started_at\"] = pd.to_datetime(sp[\"started_at\"], format='mixed', yearfirst=True, utc=True)\n",
    "sp[\"finished_at\"] = pd.to_datetime(sp[\"finished_at\"], format='mixed', yearfirst=True, utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpls = pd.read_csv(os.path.join(CONFIG[\"raw_mobis\"], \"legs.csv\"), usecols=[0, 1, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpls[\"started_at\"] = pd.to_datetime(tpls[\"started_at\"], format='mixed', yearfirst=True, utc=True)\n",
    "tpls[\"finished_at\"] = pd.to_datetime(tpls[\"finished_at\"], format='mixed', yearfirst=True, utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5152,), (5130,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpls[\"user_id\"].unique().shape, sp[\"user_id\"].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative duration records have already been dropped\n",
    "\n",
    "sp[\"duration\"] = (sp[\"finished_at\"] - sp[\"started_at\"]).dt.total_seconds()\n",
    "tpls[\"duration\"] = (tpls[\"finished_at\"] - tpls[\"started_at\"]).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = sp.sort_values(by=\"started_at\").reset_index(drop=True)\n",
    "tpls = tpls.sort_values(by=\"started_at\").reset_index(drop=True)\n",
    "\n",
    "sp.index.name = \"id\"\n",
    "tpls.index.name = \"id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _alter_diff(df):\n",
    "    df.sort_values(by=\"started_at\", inplace=True)\n",
    "    df[\"diff\"] = pd.NA\n",
    "    df[\"st_next\"] = pd.NA\n",
    "\n",
    "    diff = df[\"started_at\"].iloc[1:].reset_index(drop=True) - df[\"finished_at\"].iloc[:-1].reset_index(drop=True)\n",
    "    df[\"diff\"].iloc[:-1] = diff.dt.total_seconds()\n",
    "    df[\"st_next\"].iloc[:-1] = df[\"started_at\"].iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    df.loc[df[\"diff\"] < 0, \"finished_at\"] = df.loc[df[\"diff\"] < 0, \"st_next\"]\n",
    "\n",
    "    df[\"started_at\"], df[\"finished_at\"] = pd.to_datetime(df[\"started_at\"]), pd.to_datetime(df[\"finished_at\"])\n",
    "    df[\"duration\"] = (df[\"finished_at\"] - df[\"started_at\"]).dt.total_seconds()\n",
    "\n",
    "    # print(df.loc[df[\"diff\"] < 0])\n",
    "    df.drop(columns=[\"diff\", \"st_next\"], inplace=True)\n",
    "    df.drop(index=df[df[\"duration\"] <= 0].index, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def filter_duplicates(sp, tpls):\n",
    "\n",
    "    # merge trips and staypoints\n",
    "    sp[\"type\"] = \"sp\"\n",
    "    tpls[\"type\"] = \"tpl\"\n",
    "    df_all = pd.merge(sp, tpls, how=\"outer\")\n",
    "\n",
    "    df_all = df_all.groupby(\"user_id\", as_index=False).apply(_alter_diff)\n",
    "    sp = df_all.loc[df_all[\"type\"] == \"sp\"].drop(columns=[\"type\"])\n",
    "    tpls = df_all.loc[df_all[\"type\"] == \"tpl\"].drop(columns=[\"type\"])\n",
    "\n",
    "    sp = sp[[\"id\", \"user_id\", \"started_at\", \"finished_at\", \"geometry\", \"duration\", \"purpose\", \"detected_purpose\", \"overseas\"]]\n",
    "    tpls = tpls[[\"id\", \"user_id\", \"started_at\", \"finished_at\",\"duration\"]]\n",
    "\n",
    "    return sp.set_index(\"id\"), tpls.set_index(\"id\")\n",
    "\n",
    "sp, tpls = filter_duplicates(sp.reset_index(), tpls.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_path = os.path.join(\"data\", \"quality\")\n",
    "quality_file = os.path.join(quality_path, \"mobis_filtered.csv\")\n",
    "if Path(quality_file).is_file():\n",
    "    valid_users = pd.read_csv(quality_file)[\"user_id\"].values\n",
    "else:\n",
    "    if not os.path.exists(quality_path):\n",
    "        os.makedirs(quality_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp[\"is_activity\"] = True\n",
    "sp.loc[sp[\"purpose\"] == \"wait\", \"is_activity\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load geometry: 100%|██████████████████████████████████████████████████████| 4804194/4804194 [00:55<00:00, 86942.31it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc=\"Load geometry\")\n",
    "sp[\"geometry\"] = sp[\"geometry\"].progress_apply(wkt.loads)\n",
    "sp = gpd.GeoDataFrame(sp, crs=\"EPSG:4326\", geometry=\"geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the trackintel trip generation\n",
    "sp, tpls, trips = generate_trips(sp, tpls, add_geometry=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Generate user filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting merge (4804194, 12) (4363607, 5)\n",
      "finished merge (9167801, 15)\n",
      "**************************************************\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "5168\n",
      "final selected user 2095\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _filter_user(df, min_thres, mean_thres):\n",
    "    consider = df.loc[df[\"quality\"] != 0]\n",
    "    if (consider[\"quality\"].min() > min_thres) and (consider[\"quality\"].mean() > mean_thres):\n",
    "        return df\n",
    "\n",
    "\n",
    "def _get_tracking_quality(df, window_size):\n",
    "\n",
    "    weeks = (df[\"finished_at\"].max() - df[\"started_at\"].min()).days // 7\n",
    "    start_date = df[\"started_at\"].min().date()\n",
    "\n",
    "    quality_list = []\n",
    "    # construct the sliding week gdf\n",
    "    for i in range(0, weeks - window_size + 1):\n",
    "        curr_start = datetime.datetime.combine(start_date + datetime.timedelta(weeks=i), datetime.time())\n",
    "        curr_end = datetime.datetime.combine(curr_start + datetime.timedelta(weeks=window_size), datetime.time())\n",
    "\n",
    "        # the total df for this time window\n",
    "        cAll_gdf = df.loc[(df[\"started_at\"] >= curr_start) & (df[\"finished_at\"] < curr_end)]\n",
    "        if cAll_gdf.shape[0] == 0:\n",
    "            continue\n",
    "        total_sec = (curr_end - curr_start).total_seconds()\n",
    "\n",
    "        quality_list.append([i, cAll_gdf[\"duration\"].sum() / total_sec])\n",
    "    ret = pd.DataFrame(quality_list, columns=[\"timestep\", \"quality\"])\n",
    "    ret[\"user_id\"] = df[\"user_id\"].unique()[0]\n",
    "    return ret\n",
    "\n",
    "def calculate_user_quality(sp, trips, file_path, quality_filter):\n",
    "\n",
    "    trips[\"started_at\"] = pd.to_datetime(trips[\"started_at\"]).dt.tz_localize(None)\n",
    "    trips[\"finished_at\"] = pd.to_datetime(trips[\"finished_at\"]).dt.tz_localize(None)\n",
    "    sp[\"started_at\"] = pd.to_datetime(sp[\"started_at\"]).dt.tz_localize(None)\n",
    "    sp[\"finished_at\"] = pd.to_datetime(sp[\"finished_at\"]).dt.tz_localize(None)\n",
    "\n",
    "    # merge trips and staypoints\n",
    "    print(\"starting merge\", sp.shape, trips.shape)\n",
    "    sp[\"type\"] = \"sp\"\n",
    "    trips[\"type\"] = \"tpl\"\n",
    "    all_df = pd.concat([sp, trips])\n",
    "    print(\"finished merge\", all_df.shape)\n",
    "    print(\"*\" * 50)\n",
    "    all_df = _split_overlaps(all_df, granularity=\"day\")\n",
    "    all_df[\"duration\"] = (all_df[\"finished_at\"] - all_df[\"started_at\"]).dt.total_seconds()\n",
    "\n",
    "    print(len(all_df[\"user_id\"].unique()))\n",
    "\n",
    "    # get quality\n",
    "    total_quality = temporal_tracking_quality(all_df, granularity=\"all\")\n",
    "    # get tracking days\n",
    "    total_quality[\"days\"] = (\n",
    "        all_df.groupby(\"user_id\").apply(lambda x: (x[\"finished_at\"].max() - x[\"started_at\"].min()).days).values\n",
    "    )\n",
    "    # filter based on days\n",
    "    user_filter_day = (\n",
    "        total_quality.loc[(total_quality[\"days\"] > quality_filter[\"day_filter\"])]\n",
    "        .reset_index(drop=True)[\"user_id\"]\n",
    "        .unique()\n",
    "    )\n",
    "    # filter based on sliding quality\n",
    "    sliding_quality = (\n",
    "        all_df.groupby(\"user_id\")\n",
    "        .apply(_get_tracking_quality, window_size=quality_filter[\"window_size\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    filter_after_day = sliding_quality.loc[sliding_quality[\"user_id\"].isin(user_filter_day)]\n",
    "\n",
    "    if \"min_thres\" in quality_filter:\n",
    "        # filter based on quanlity\n",
    "        filter_after_day = (\n",
    "            filter_after_day.groupby(\"user_id\")\n",
    "            .apply(_filter_user, min_thres=quality_filter[\"min_thres\"], mean_thres=quality_filter[\"mean_thres\"])\n",
    "            .reset_index(drop=True)\n",
    "            .dropna()\n",
    "        )\n",
    "\n",
    "    filter_after_user_quality = filter_after_day.groupby(\"user_id\", as_index=False)[\"quality\"].mean()\n",
    "\n",
    "    print(\"final selected user\", filter_after_user_quality.shape[0])\n",
    "    filter_after_user_quality.to_csv(file_path, index=False)\n",
    "    return filter_after_user_quality[\"user_id\"].values\n",
    "\n",
    "quality_filter = {\"day_filter\": 50, \"window_size\": 5, \"min_thres\": 0.5, \"mean_thres\": 0.6}\n",
    "valid_users = calculate_user_quality(sp.copy(), trips.copy(), quality_file, quality_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter valid users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = sp.loc[sp[\"user_id\"].isin(valid_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_activity\n",
       "True     1483849\n",
       "False      73295\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp[\"is_activity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before spatial filtering:  1557144\n",
      "After spatial filtering:  1440822\n"
     ]
    }
   ],
   "source": [
    "def _filter_within_swiss(stps, swissBound):\n",
    "    \"\"\"Spatial filtering of staypoints.\"\"\"\n",
    "    # save a copy of the original projection\n",
    "    init_crs = stps.crs\n",
    "    # project to projected system\n",
    "    stps = stps.to_crs(swissBound.crs)\n",
    "\n",
    "    ## parallel for speeding up\n",
    "    stps[\"within\"] = _apply_parallel(stps[\"geometry\"], _apply_extract, swissBound)\n",
    "    sp_swiss = stps[stps[\"within\"] == True].copy()\n",
    "    sp_swiss.drop(columns=[\"within\"], inplace=True)\n",
    "\n",
    "    return sp_swiss.to_crs(init_crs)\n",
    "    \n",
    "def _apply_extract(df, swissBound):\n",
    "    \"\"\"The func for _apply_parallel: judge whether inside a shp.\"\"\"\n",
    "    tqdm.pandas(desc=\"pandas bar\")\n",
    "    shp = swissBound[\"geometry\"].to_numpy()[0]\n",
    "    return df.progress_apply(lambda x: shp.contains(x))\n",
    "\n",
    "\n",
    "def _apply_parallel(df, func, other, n=-1):\n",
    "    \"\"\"parallel apply for spending up.\"\"\"\n",
    "    if n is None:\n",
    "        n = -1\n",
    "    dflength = len(df)\n",
    "    cpunum = multiprocessing.cpu_count()\n",
    "    if dflength < cpunum:\n",
    "        spnum = dflength\n",
    "    if n < 0:\n",
    "        spnum = cpunum + n + 1\n",
    "    else:\n",
    "        spnum = n or 1\n",
    "\n",
    "    sp = list(range(dflength)[:: int(dflength / spnum + 0.5)])\n",
    "    sp.append(dflength)\n",
    "    slice_gen = (slice(*idx) for idx in zip(sp[:-1], sp[1:]))\n",
    "    results = Parallel(n_jobs=n, verbose=0)(delayed(func)(df.iloc[slc], other) for slc in slice_gen)\n",
    "    return pd.concat(results)\n",
    "\n",
    "swissBoundary = gpd.read_file(os.path.join(\".\", \"data\", \"swiss\", \"swiss_1903+.shp\"))\n",
    "print(\"Before spatial filtering: \", sp.shape[0])\n",
    "sp_swiss = _filter_within_swiss(sp, swissBoundary)\n",
    "print(\"After spatial filtering: \", sp_swiss.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter activity staypoints\n",
    "sp_swiss = sp_swiss.loc[sp_swiss[\"is_activity\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate locations\n",
    "sp_swiss, locs = sp_swiss.as_staypoints.generate_locations(\n",
    "    epsilon=20, num_samples=2, distance_metric=\"haversine\", agg_level=\"dataset\", n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filter non-location staypoints:  1217863\n"
     ]
    }
   ],
   "source": [
    "# filter noise staypoints\n",
    "sp_swiss = sp_swiss.loc[~sp_swiss[\"location_id\"].isna()].copy()\n",
    "print(\"After filter non-location staypoints: \", sp_swiss.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location size:  62673 62673\n"
     ]
    }
   ],
   "source": [
    "# save locations\n",
    "locs = locs[~locs.index.duplicated(keep=\"first\")]\n",
    "filtered_locs = locs.loc[locs.index.isin(sp_swiss[\"location_id\"].unique())]\n",
    "\n",
    "# locations without duplication, user_id have no meaning\n",
    "filtered_locs.as_locations.to_csv(os.path.join(\".\", \"data\", f\"locations.csv\"))\n",
    "print(\"Location size: \", sp_swiss[\"location_id\"].unique().shape[0], filtered_locs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After staypoints merging:  1197153\n"
     ]
    }
   ],
   "source": [
    "sp_swiss = sp_swiss[[\"user_id\", \"started_at\", \"finished_at\", \"geometry\", \"location_id\"]]\n",
    "# merge staypoints\n",
    "sp_merged = sp_swiss.as_staypoints.merge_staypoints(\n",
    "    triplegs=pd.DataFrame([]), max_time_gap=\"1min\", agg={\"location_id\": \"first\"}\n",
    ")\n",
    "print(\"After staypoints merging: \", sp_merged.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_merged[\"duration\"] = (sp_merged[\"finished_at\"] - sp_merged[\"started_at\"]).dt.total_seconds() // 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User size:  2094\n"
     ]
    }
   ],
   "source": [
    "print(\"User size: \", sp_merged[\"user_id\"].unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_merged.to_csv(os.path.join(\".\", \"data\", f\"sp_filtered.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "659ffd3ea00dbbc52f857660b8dafea05f804bc55dd91047cefb31e38b5505f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
